# âš™ï¸ AMBIENTE DE EXECUÃ‡ÃƒO

## ğŸ¯ PrÃ©-requisitos e ConfiguraÃ§Ã£o

### Ambiente de Desenvolvimento:

- **Sistema Operacional**: Windows 11
- **Compilador**: g++ (MSYS2 UCRT64)
- **MPI**: Microsoft MPI (MS-MPI)
- **IDE**: Visual Studio Code
- **Hardware**: 8 cores fÃ­sicos, 16 threads lÃ³gicos

## ğŸ› ï¸ InstalaÃ§Ã£o do C:

### Instalar MSYS2:

```bash
pacman -S mingw-w64-ucrt-x86_64-gcc
pacman -S mingw-w64-ucrt-x86_64-gdb
```

**Link de download:** https://www.msys2.org/

## ğŸ“¥ Download do MPI

### Arquivos NecessÃ¡rios:

1. **msmpisetup.exe** - Runtime do MPI
2. **msmpisdk.msi** - SDK para desenvolvimento

**Link de download:** https://www.microsoft.com/en-us/download/details.aspx?id=105289

---

## ğŸ› ï¸ Instalar MPI

### 1. Instalar Runtime (msmpisetup.exe)

- Executar como Administrador
- Durante instalaÃ§Ã£o, alterar diretÃ³rio para: `C:\MPI`
- Confirmar instalaÃ§Ã£o

### 2. Instalar SDK (msmpisdk.msi)

- Executar como Administrador  
- Durante instalaÃ§Ã£o, alterar diretÃ³rio para: `C:\MPI\SDK`
- Confirmar instalaÃ§Ã£o

---

## ğŸ”§ Configurar VariÃ¡veis de Ambiente

### Via CMD (como Administrador):

```cmd
setx MSMPI_INC "C:\MPI\SDK\Include" /M
setx MSMPI_LIB64 "C:\MPI\SDK\Lib\x64" /M
setx PATH "%PATH%;C:\MPI\Bin" /M
```

# ğŸ“Š RELATÃ“RIO COMPLETO

## ğŸ¯ Resumo Executivo

Este relatÃ³rio apresenta uma anÃ¡lise abrangente da escalabilidade do algoritmo MergeSort paralelo implementado com MPI, testando desde 2 atÃ© 2048 processos com um array de 2.097.152 elementos. Os resultados demonstram claramente o ponto Ã³timo de paralelizaÃ§Ã£o e os limites prÃ¡ticos da escalabilidade.

---

## ğŸ“ˆ Resultados de Speedup por NÃºmero de Processos

| Processos | Speedup | EficiÃªncia | Tempo Paralelo | ClassificaÃ§Ã£o |
|-----------|---------|------------|----------------|---------------|
| **2**     | **1.79x** | **89.5%** | 0.146s | ğŸŸ¢ **EXCELENTE** |
| **4**     | **2.98x** | **74.5%** | 0.089s | ğŸŸ¢ **EXCELENTE** |
| **8**     | **3.32x** | **41.5%** | 0.080s | ğŸŸ¢ **MUITO BOM** |
| **16**    | **3.92x** | **24.5%** | 0.072s | ğŸŸ¢ **Ã“TIMO** |
| **32**    | **3.38x** | **10.6%** | 0.083s | ğŸŸ¡ **BOM** |
| **64**    | **2.64x** | **4.1%**  | 0.109s | ğŸŸ  **REGULAR** |
| **128**   | **1.91x** | **1.5%**  | 0.171s | ğŸŸ  **FRACO** |
| **256**   | **0.91x** | **0.4%**  | 0.425s | ğŸ”´ **CONTRAPRODUCENTE** |
| **512**   | **0.51x** | **0.1%**  | 0.775s | ğŸ”´ **MUITO RUIM** |
| **1024**  | **0.19x** | **0.02%** | 1.862s | ğŸ”´ **CATASTRÃ“FICO** |
| **2048**  | **0.05x** | **0.002%** | 7.767s | ğŸ”´ **INVIÃVEL** |

---

### Tempo Paralelo x NÃºmero De Processos

<img width="670" height="480" alt="image" src="https://github.com/user-attachments/assets/83a12274-d8df-4336-b6b6-a7c6fc460fff" />

### ğŸ“Š EvoluÃ§Ã£o dos Gargalos (%)

| Processos | OrdenaÃ§Ã£o | Scatter | Gather | Merge Final | ComunicaÃ§Ã£o Total |
|-----------|-----------|---------|--------|-------------|-------------------|
| **2**     | **91.0%** | 3.4%    | 0.6%   | 5.0%        | **4.0%** |
| **4**     | **75.0%** | 6.8%    | 1.9%   | 16.2%       | **8.7%** |
| **8**     | **61.5%** | 8.2%    | 2.6%   | 27.5%       | **10.8%** |
| **16**    | **47.0%** | 10.6%   | 3.4%   | 38.8%       | **14.0%** |
| **32**    | **37.8%** | 11.2%   | 3.9%   | 42.2%       | **15.1%** |
| **64**    | **16.1%** | 13.9%   | 7.0%   | 40.9%       | **20.9%** |
| **128**   | **3.1%**  | 15.4%   | 21.7%  | 32.4%       | **37.1%** |
| **256**   | **1.8%**  | 15.1%   | 35.1%  | 21.9%       | **50.2%** |
| **512**   | **0.2%**  | 22.5%   | 42.7%  | 12.8%       | **65.2%** |
| **1024**  | **0.1%**  | 48.6%   | 30.3%  | 6.7%        | **78.9%** |
| **2048**  | **0.0%**  | 76.2%   | 15.4%  | 1.7%        | **91.6%** |

---

### EvoluÃ§Ã£o Dos Gargalos

<img width="670" height="480" alt="image" src="https://github.com/user-attachments/assets/b49a1899-7851-4cea-bc90-cccf9f5dd4cb" />

## ğŸ¯ Pontos-Chave Identificados

### ğŸ† Ponto Ã“timo: 16 Processos

- **MÃ¡ximo speedup**: 3.92x
- **EficiÃªncia**: 24.5%
- **Trabalho Ãºtil**: 47.0%
- **ComunicaÃ§Ã£o**: 14.0%

### âš–ï¸ Ponto de EquilÃ­brio: 32 Processos

- Ãšltimo ponto com speedup > 3.0x
- Merge final torna-se gargalo dominante (42.2%)

### ğŸ“‰ InÃ­cio da DegradaÃ§Ã£o: 64 Processos

- Speedup cai para 2.64x
- ComunicaÃ§Ã£o MPI torna-se significativa (20.9%)

### ğŸš¨ Ponto de Ruptura: 256 Processos

- **Speedup < 1.0** (paralelo mais lento que sequencial)
- ComunicaÃ§Ã£o domina 50.2% do tempo

### ğŸ’¥ Colapso Total: 1024+ Processos

- Speedup catastrÃ³fico (0.19x e 0.05x)
- ComunicaÃ§Ã£o consome 78.9% e 91.6% do tempo

---

## ğŸ“Š AnÃ¡lise de Gargalos por Categoria

### ğŸ”µ Zona Eficiente (2-16 processos)

- **CaracterÃ­stica**: OrdenaÃ§Ã£o domina (47-91%)
- **ComunicaÃ§Ã£o**: Baixa (4-14%)
- **Speedup**: Crescente (1.79x â†’ 3.92x)
- **ConclusÃ£o**: Paralelismo efetivo

### ğŸŸ¡ Zona de TransiÃ§Ã£o (32-64 processos)

- **CaracterÃ­stica**: Merge final vira gargalo principal
- **ComunicaÃ§Ã£o**: Moderada (15-21%)
- **Speedup**: EstÃ¡vel/declinante (3.38x â†’ 2.64x)
- **ConclusÃ£o**: Ainda Ãºtil, mas com limitaÃ§Ãµes

### ğŸŸ  Zona de DegradaÃ§Ã£o (128-256 processos)

- **CaracterÃ­stica**: ComunicaÃ§Ã£o supera trabalho Ãºtil
- **ComunicaÃ§Ã£o**: Alta (37-50%)
- **Speedup**: DeclÃ­nio acentuado (1.91x â†’ 0.91x)
- **ConclusÃ£o**: Ineficiente

### ğŸ”´ Zona de Colapso (512+ processos)

- **CaracterÃ­stica**: ComunicaÃ§Ã£o domina completamente
- **ComunicaÃ§Ã£o**: CrÃ­tica (65-92%)
- **Speedup**: CatastrÃ³fico (0.51x â†’ 0.05x)
- **ConclusÃ£o**: Contraproducente

---

## ğŸ”¬ ValidaÃ§Ã£o da Lei de Amdahl

### Parte Sequencial Identificada:

- **Merge Final**: 5% â†’ 42% do tempo total
- **Limite teÃ³rico**: ~2.4x speedup mÃ¡ximo
- **Resultado obtido**: 3.92x (superou devido a cache effects)

### Overhead de ComunicaÃ§Ã£o:

- **Scatter**: Cresce de 3.4% â†’ 76.2%
- **Gather**: Cresce de 0.6% â†’ 15.4%
- **Total**: Crescimento exponencial com nÃºmero de processos

---

## ğŸ“‹ RecomendaÃ§Ãµes

### âœ… Para MÃ¡xima Performance:

- **Usar 8-16 processos** para arrays de ~2M elementos
- **Speedup esperado**: 3.3x - 3.9x
- **EficiÃªncia**: 20-40%

### âš–ï¸ Para Balancear Recursos:

- **Usar 4-8 processos** para eficiÃªncia alta
- **Speedup esperado**: 2.98x - 3.32x
- **EficiÃªncia**: 40-75%

### âŒ Evitar:

- **32+ processos** para arrays desta magnitude
- **256+ processos** (sempre contraproducente)

---

## ğŸ† ConclusÃµes

1. **Escalabilidade Limitada**: MergeSort paralelo escala eficientemente apenas atÃ© 16 processos
2. **Gargalo Principal**: Merge final sequencial limita speedup mÃ¡ximo
3. **Overhead CrÃ­tico**: ComunicaÃ§Ã£o MPI cresce exponencialmente
4. **Granularidade**: Arrays de 2M elementos sÃ£o insuficientes para muitos processos
5. **Lei de Amdahl Validada**: Parte sequencial e overhead limitam paralelizaÃ§Ã£o

## ğŸ“š ContribuiÃ§Ã£o

Este estudo fornece dados empÃ­ricos precisos sobre os limites prÃ¡ticos da paralelizaÃ§Ã£o do MergeSort, demonstrando que **mais processos nÃ£o garantem melhor performance** e validando teorias fundamentais de computaÃ§Ã£o paralela atravÃ©s de mediÃ§Ãµes experimentais rigorosas.

---

**ğŸ¯ Resultado Final: Ponto Ã³timo de 16 processos com speedup de 3.92x para arrays de 2.097.152 elementos.**
